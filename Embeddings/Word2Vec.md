### Word2Vec

#### word2vec 简介

Word2vec是 Google在2013年开源的一款将词表征为实数值向量的高效工具,其利用深度学习的思想，可以通过训练，把对文本内容的处理简化为K维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。Word2vec输出的词向量可以被用来做很多NLP相关的工作，比如聚类、找同义词、词性分析等等。如果换个思路，把词当做特征，那么Word2vec就可以把特征映射到K维向量空间，可以为文本数据寻求更加深层次的特征表示。

Word2vec 使用的是Distributed representation的词向量表示方式。Distributed representation最早由 Hinton在1986年提出[4]。其基本思想是通过训练将每个词映射成K维实数向量（K一般为模型中的超参数），通过词之间的距离(比如cosine相似度、欧氏距离等）来判断它们之间的语义相似度。其采用一个三层的神经网络，输入层-隐层-输出层。有个核心的技术是根据词频用Huffman编码，使得所有词频相似的词隐藏层激活的内容基本一致，出现频率越高的词语，他们激活的隐藏层数目越少，这样有效的降低了计算的复杂度。而Word2vec大受欢迎的一个原因正是其高效性，Mikolov在论文[2]中指出，一个优化的单机版本一天可训练上千亿词。

#### 模型分析

如下图所示，Word2Vec使用三层神经网络本身是对语言模型进行建模，同时获得一种单词在向量空间上的表示 ，而这个副作用才是Word2vec的真正目标。

![skip_gram_net_arch](/Users/zhaozhichao/Desktop/NLP/Embeddings/assets/skip_gram_net_arch.png)

输入是什么：

输出是什么：



副作用是什么作用：



两种模型：

![word2vec_architectures](/Users/zhaozhichao/Desktop/NLP/Embeddings/assets/word2vec_architectures.png)

数学原理：





和神经网络的作用：

加快的Huffman Coding



传统的统计语言模型是表示语言基本单位（一般为句子）的概率分布函数，这个概率分布也就是该语言的生成模型。一般语言模型可以使用各个词语条件概率的形式表示：

p(s)=p(w_1^T )=p(w_1,w_2,…,w_T )=∏_t p(w_t |context)

Word2vec采用的是**层次化Log-Bilinear语言模型**，其中一种是CBOW(Continuous Bag-of-Words Model)模型，由上下文预测下一个词为w_t的公式为：

p(w_t |context)=p(w_t |w_(t-k),w_(t-k+1),…,w_(t-1),w_(t+1),…,w_(t+k-1),w_(t+k))

CBOW的计算可以用 层次Softmax算法 ，这种算法结合了Huffman编码，每个词 w 都可以从树的根结点root沿着唯一一条路径被访问到，其路径也就形成了其编码code。假设 n(w, j)为这条路径上的第 j 个结点，且 L(w)为这条路径的长度， j 从 1 开始编码，即 n(w, 1)=root，n(w, L(w)) = w。对于第 j 个结点，层次 Softmax 定义的Label 为 1 - code[j]。

文本深度表示模型Word2Vec

取一个适当大小的窗口当做语境，输入层读入窗口内的词，将它们的向量（K维，初始随机）加和在一起，形成隐藏层K个节点。输出层是一个巨大的二叉 树，叶节点代表语料里所有的词（语料含有V个独立的词，则二叉树有|V|个叶节点）。而这整颗二叉树构建的算法就是Huffman树。这样，对于叶节点的 每一个词，就会有一个全局唯一的编码，形如"010011"，不妨记左子树为1，右子树为0。接下来，隐层的每一个节点都会跟二叉树的内节点有连边，于是 对于二叉树的每一个内节点都会有K条连边，每条边上也会有权值。
文本深度表示模型Word2Vec
word2vec_hs_network.jpg 750x740 73.3 KB

对于语料库中的某个词w_t，对应着二叉树的某个叶子节点，因此它必然有一个二进制编码，如"010011"。在训练阶段，当给定上下文，要预测后 面的词w_t的时候，我们就从二叉树的根节点开始遍历，这里的目标就是预测这个词的二进制编号的每一位。即对于给定的上下文，我们的目标是使得预测词的二 进制编码概率最大。形象地说，我们希望在根节点，词向量和与根节点相连经过logistic计算得到bit=1的概率尽量接近0，在第二层，希望其 bit=1的概率尽量接近1，这么一直下去，我们把一路上计算得到的概率相乘，即得到目标词w_t在当前网络下的概率P(w_t)，那么对于当前这个 sample的残差就是1-P(w_t)，于是就可以使用梯度下降法训练这个网络得到所有的参数值了。显而易见，按照目标词的二进制编码计算到最后的概率 值就是归一化的。

Hierarchical Softmax用Huffman编码构造二叉树，其实借助了分类问题中，使用一连串二分类近似多分类的思想。例如我们是把所有的词都作为输出，那么“桔 子”、“汽车”都是混在一起。给定w_t的上下文，先让模型判断w_t是不是名词，再判断是不是食物名，再判断是不是水果，再判断是不是“桔子”。

但是在训练过程中，模型会赋予这些抽象的中间结点一个合适的向量，这个向量代表了它对应的所有子结点。因为真正的单词公用了这些抽象结点的向量，所 以Hierarchical Softmax方法和原始问题并不是等价的，但是这种近似并不会显著带来性能上的损失同时又使得模型的求解规模显著上升。

没有使用这种二叉树，而是直接从隐层直接计算每一个输出的概率——即传统的Softmax，就需要对|V|中的每一个词都算一遍，这个过程时间复杂 度是O(|V|)的。而使用了二叉树（如Word2vec中的Huffman树），其时间复杂度就降到了O(log2(|V|))，速度大大地加快了。 